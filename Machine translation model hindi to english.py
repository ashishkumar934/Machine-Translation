# -*- coding: utf-8 -*-
"""Machine translation encoder decoder model of 100K data set.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r2t-ioFGragkhxZc18CHUl6WkIIBhESK
"""

#Gpu Connection Check
import tensorflow as tf
device_name=tf.test.gpu_device_name()
if device_name!='/device:GPU:0':
  raise SystemError('Gpu Not Found')
print('found gpu', device_name)

#importing libraries

import math
import numpy as np
import tensorflow as tf
import keras
import os
import collections
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Input, Embedding, Dense, GRU,Bidirectional,RepeatVector,TimeDistributed,LSTM
from keras.models import Model,Sequential
from keras.optimizers import Adam,RMSprop

#reading english sentecnes files from the drive
with open("/content/drive/My Drive/pruned_train.en",'r') as file:
  english_data=file.read()
english_data=english_data.lower()
english_list=english_data.split('\n')
file.close()

#printing first 10 sentences
i=0
while i<10:
  print(english_list[i])
  i+=1

i=0
while i<10:
  print(hindi_list[i])
  i+=1

from google.colab import drive
drive.mount('/content/drive')

#reading hindi_sentecnes from the drive
with open("/content/drive/My Drive/pruned_train.hi",'r') as file:
  hindi_data=file.read()
hindi_list=hindi_data.split('\n')
file.close()

#preprocessing data by removing the punctuation marks and special data
def remove_punctuation(txt):
  txt_output=""
  ty="'"
  ts="s"
  gh=0
  cnt=len(txt)
  for count,word in enumerate(txt):
    if gh is 1:
      gh=0
      continue
    if word in ty:
      if count+1<cnt and txt[count+1] in ts:
        gh=1
        continue 
    if word not in '''!"#$%&'()*+,-./:;”“<=>?@[\]^_`{|}~1234567890''':
      txt_output+=word
  return txt_output

#punctuation marks removal from hindi dataset
cleaned_hindi_list=[]
for list in hindi_list:
  cleaned_hindi_list.append(remove_punctuation(list))

#punctuation marks removal from english dataset
cleaned_english_list=[]
for list in english_list:
  cleaned_english_list.append(remove_punctuation(list))

cleaned_hindi_list[5]

cleaned_english_list[5]

hindi_word_count=collections.Counter([word for list in cleaned_hindi_list for word in list.split()])
print(len(hindi_word_count))
print(hindi_word_count.most_common(3000))

#list of the most commmon words in hindi that are to be removed
remove=['के','है','और','में','की','से','को','का','हैं','कि','पर','लिए','तो','एक','जो','1','2','।','3','4','5','6','7','8','9','0','१','२','३','४','५','६','७','८','९','०','नहीं','भी','हो','ही','किया','यह','ने','कर','वह','इस','करने','अपने','था','न','तुम','या','वे','कुछ','गया','कोई','उनके','तथा','थे','साथ','लोग','हम','लोगों','किसी','करते','जब','दिया','अल्लाह','रूप','फिर','उसके','ये','क्या','द्वारा','तक','अपनी','जाता','उन्हें','होता','जा','उसे','उन','उस','रहे','करता','थी','करना','सकता','मैं','हुए','करें','बहुत','दो','हमने','आप','तुम्हारे', 'रहा', 'समय','उनकी','पास','बात','बाद', 'यदि','तीसरी','तरह','सी','बी', 'पहले','हुआ','अधिक','कहा','वाले','ओर','उन्होंने','उसकी','होगा','होती','होने','गई','दी','चाहिए','गए','इसके','अगर','सभी','हमारे', 'ऐसे','इन','तुम्हें', 'करो', 'जाने','दे','ऐसा','कार्य','उसने','लेकिन','लिया','किए','अन्य','जिस','एवं','हुई', 'जाती', 'मुझे', 'ले', 'अब','होते', 'कम','उसका','सब', 'ताकि', 'व','उनका','वही', 'हर','बारे','वाला']

with open('/content/final_stopwords.txt','r') as file:
  data=file.read()
remove_these_hindi_words=data.split('\n')
remove_these_hindi_words[-1]
remove_these_hindi_words+=remove

english_word_count=collections.Counter([word for list in cleaned_english_list for word in list.split()])
print(len(english_word_count))
print(english_word_count.most_common(1000))

#list of the english words to be removed from the dataset
remove_these_english_words=['the','of','b','c','and','st','nd','rd','th','1','2','3','4','5','6','7','8','9','0','ll','m','i','to','in','a','is','for','that','you','it','not','be','he','they','with','are','on','as','this','have','was','from','by','we','them','his','or','will','who','which','their','has','but','i','an','all','allah','s','your','at','him','if','those','so','when','do','one','there','had','what','were','been','can','people','no','then','our','its','may','also','would','only','other','any','shall','some','these','my','day','should','out','more','into','made','said','t','than','such','over','most','could','did','even','under','being','because','upon','very','she','while','ye']

i=0
while i<10:
  print(cleaned_english_list[i])
  i+=1

i=0
for list in cleaned_english_list:
  for word in list.split():
    print(word)
  print()
  i+=1
  if(i>10):
    break

#removing words from english dataset
final_english_list=[]
for list in cleaned_english_list:
  txt=''
  for word in list.split():
    if word not in remove_these_english_words:
      txt+=word+' '
  final_english_list.append(txt)

final_english_list[1]

i=0
for list in final_english_list:
  for word in list.split():
    print(word)
  print()
  i+=1
  if(i>10):
    break

i=0
while i<10:
  print(final_english_list[i])
  i+=1

i=0
for list in cleaned_hindi_list:
  for word in list.split():
    print(word)
  print()
  i+=1
  if(i>10):
    break

#removing words from the hindi list
final_hindi_list=[]
for list in cleaned_hindi_list:
  txt=''
  for word in list.split():
    if word not in remove_these_hindi_words:
      txt+=word+' '
  final_hindi_list.append(txt)

final_hindi_list[650456]

i=0
for list in final_hindi_list:
  for word in list.split():
    print(word)
  print()
  i+=1
  if(i>10):
    break

i=0
while i<10:
  print(final_hindi_list[i])
  i+=1

#function for reversing the list
def reversing(txt):
  txt2=[]
  txt2 = txt[::-1]
  return txt2

num_words=25000

#adding starting and ending position in each english sentence
start_position="start "
end_position=" endlj"
print(end_position)

final_english_list3=[]
for list in final_english_list:
  final_list=start_position+list+end_position
  final_english_list3.append(final_list)

final_english_list3[8]

final_english_list4=final_english_list3[0:100000]
final_hindi_list4=final_hindi_list[0:100000]

#making Tokenizing class having Keras Tokenization class with some additional functions 
class Tokenizing_Text(Tokenizer):
  def __init__(self,texts,padding,reverse=False,num_words=None):
    #Inheritance of the Tokenizer class from keras 
    Tokenizer.__init__(self,num_words=num_words,char_level=False)

    self.fit_on_texts(texts)

    self.index_to_words=dict(zip(self.word_index.values(),
                                 self.word_index.keys()))
    self.tokens=self.texts_to_sequences(texts)

    if reverse:
      for count,g in enumerate(self.tokens,0):
        self.tokens[count]=reversing(g)
      truncating='pre'
    
    else:
      truncating='post'
    
    self.num_tokens = [len(g) for g in self.tokens]
    self.max_tokens = 24
    
    self.tokens_padded=pad_sequences(self.tokens,
                                     maxlen=self.max_tokens,
                                     padding=padding,
                                     truncating=truncating)
    
#function for converting tokens of the language to words
  def token_to_word(self,token):
      word = " " if token == 0 else self.index_to_words[token]
      return word
# function for making sentence out of gicen tokens of the words
  def tokens_to_string(self,tokens):
      words=[self.index_to_words[token] for token in tokens if token!=0]
      text = " ".join(words)

      return text
    
#function for converting the text to the tokens 
  def text_to_tokens(self, text, reverse=False, padding=False):

      tokens=self.texts_to_sequences([text])
      tokens=np.array(tokens)

      if reverse:

        tokens=np.flip(tokens,axis=1)
        truncating='pre'

      else:
        truncating='post'

      if padding:
        tokens=pad_sequences(tokens,maxlen=self.max_tokens,padding='pre',truncating=truncating)

        return tokens

english_tokenized2=Tokenizing_Text(texts=final_english_list4,padding='post',reverse=False,num_words=num_words)

hindi_tokenized2=Tokenizing_Text(texts=final_hindi_list4,padding='pre',reverse=True,num_words=num_words)

hindi_tokens2=hindi_tokenized2.tokens_padded
english_tokens2=english_tokenized2.tokens_padded
print(hindi_tokens2.shape)
print(english_tokens2.shape)

starting_token2=english_tokenized2.word_index[start_position.strip()]
print(starting_token2)

hindi_tokens2[1]

english_tokens2[1]

ending_token2=english_tokenized2.word_index[end_position.strip()]

print(ending_token2)

english_tokenized2.tokens_to_string(english_tokens2[45])

hindi_tokenized2.tokens_to_string(hindi_tokens2[45])

# making the encoder decoder model

encoder_input_data=hindi_tokens2
hindi_tokens2.shape

decoder_input_data=english_tokens2[:,:-1]
decoder_output_data=english_tokens2[:,1:]

decoder_output_data.shape

decoder_output_data=np.reshape(decoder_output_data,(100000,23,1))
print(encoder_input_data.shape)
print(decoder_input_data.shape)
print(decoder_output_data.shape)

encoder_input = Input(shape=(None, ),name="encoder_input")

embedding_size=128

# defining Embedding layer for encoder input
encoder_embedding=Embedding(input_dim=num_words,
                            output_dim=embedding_size,
                            name='encoder_embedding')

state_size=512

# making 3 layers of Gated Recurrent Unit
encoder_gru1 = GRU(state_size, name='encoder_gru1',
                   return_sequences=True)
encoder_gru2 = GRU(state_size, name='encoder_gru2',
                   return_sequences=True)
# last layer will not return the sequence as the single vector will be fed as initial state into the decoder 
encoder_gru3 = GRU(state_size, name='encoder_gru3',
                   return_sequences=False)

#connecting various layers of the encoder
def connect_encoder():
  net=encoder_input
  net=encoder_embedding(net)
  net=encoder_gru1(net)
  net=encoder_gru2(net)
  net=encoder_gru3(net)

  encoder_output=net
  return encoder_output

encoder_output=connect_encoder()

decoder_initial_state = Input(shape=(state_size,),
                              name='decoder_initial_state')

decoder_input = Input(shape=(None, ), name='decoder_input')

#making the embedding layer for decoder input
decoder_embedding = Embedding(input_dim=num_words,
                              output_dim=embedding_size,
                              name='decoder_embedding')

decoder_gru1 = GRU(state_size, name='decoder_gru1',
                   return_sequences=True)
decoder_gru2 = GRU(state_size, name='decoder_gru2',
                   return_sequences=True)
decoder_gru3 = GRU(state_size, name='decoder_gru3',
                   return_sequences=True)

decoder_dense = Dense(num_words,
                      activation='softmax',
                      name='decoder_output')

def connect_decoder(initial_state):
    net = decoder_input
    net = decoder_embedding(net)
    net = decoder_gru1(net, initial_state=initial_state)
    net = decoder_gru2(net, initial_state=initial_state)
    net = decoder_gru3(net, initial_state=initial_state)

    decoder_output = decoder_dense(net)
    return decoder_output

decoder_output = connect_decoder(initial_state=encoder_output)

model_train = Model(inputs=[encoder_input, decoder_input],
                    outputs=[decoder_output])

model_encoder = Model(inputs=[encoder_input],
                      outputs=[encoder_output])

decoder_output = connect_decoder(initial_state=decoder_initial_state)

model_decoder = Model(inputs=[decoder_input, decoder_initial_state],
                      outputs=[decoder_output])

from keras.optimizers import RMSprop
model_train.compile(optimizer=RMSprop(lr=0.003),
                    loss='sparse_categorical_crossentropy')

x_data = \
{
    'encoder_input': encoder_input_data,
    'decoder_input': decoder_input_data
}

y_data = \
{
    'decoder_output': decoder_output_data
}

#training our model with 25 epochs and validation set 10%
model_train.fit(x=x_data,
                y=y_data,
                batch_size=512,
                epochs=25,
                validation_split=0.1)

#saving the model
model_train.save('model_train.h5')

#function for translating the text where input is hindi sentences and predicted and actual english sentences are shown
def translating(i_text, true_o_text=None):

    i_tokens = hindi_tokenized2.text_to_tokens(text=i_text,
                                                reverse=True,
                                                padding=True)
    
 
    initial_state = model_encoder.predict(i_tokens)


    max_tokens = english_tokenized2.max_tokens

    print(max_tokens)
    shape = (1, max_tokens)
    decoder_input_data = np.zeros(shape=shape, dtype=np.int)

 
    token_int = starting_token2


    o_text = ''

    count_tokens = 0
    while token_int != ending_token2 and count_tokens < max_tokens:
        decoder_input_data[0, count_tokens] = token_int
        x_data = \
        {
            'decoder_initial_state': initial_state,
            'decoder_input': decoder_input_data
        }

        decoder_output = model_decoder.predict(x_data)

        token_onehot = decoder_output[0, count_tokens, :]  
        token_int = np.argmax(token_onehot)
        sampled_word = english_tokenized2.token_to_word(token_int)
        if sampled_word is "endj":
          break
        o_text += " " + sampled_word
        count_tokens += 1

    output_tokens = decoder_input_data[0]
    
    print("Input text:")
    print(i_text)
    print()

    
    print("Translated text:")
    print(o_text)
    print()


    if true_o_text is not None:
        print("True output text:")
        print(true_o_text)
        print()
    return o_text,true_o_text

#making reference and hypothesis for the blue score calculation
def making_reference_hypothesis(txt):
  txt_output=[]
  for words in txt.split():
    txt_output.append(words)
  return txt_output

translating(i_text=final_hindi_list4[5],true_o_text=final_english_list4[5])

hypothesis = making_reference_hypothesis('first century built temple built built temple built built present century endlj')
reference = making_reference_hypothesis('start first temple built emperor ashoka century present temple dates centuries  endlj')
#there may be several references
BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)
print(BLEUscore)

translating(i_text=final_hindi_list4[8967],true_o_text=final_english_list4[8967])

translating(i_text='मृत्यु',true_o_text='die')

final_hindi_list[356234]

translating(i_text="फ़िरऔन सेना पीछा अन्ततः पानी उनपर छा जैसाकि उनपर छा जाना",true_o_text=final_english_list4[35623])

hypothesis = making_reference_hypothesis('pharaoh pursued army sea pursued sea endlj')
reference = making_reference_hypothesis('pursued troops whereat engulfed engulfed sea  endlj')
#there may be several references
BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)
print(BLEUscore)

translating(i_text=final_hindi_list[3402],true_o_text=final_english_list4[3402])

translating(i_text=final_hindi_list[3400],true_o_text=final_english_list4[3400])

translating(i_text=final_hindi_list[2500],true_o_text=final_english_list4[2500])

hypothesis = making_reference_hypothesis('mixed mixed number mixed number endlj')
reference = making_reference_hypothesis('start mixed number simplify write answer  endlj')
#there may be several references
BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)
print(BLEUscore)

translating(i_text=final_hindi_list[23],true_o_text=final_english_list[23])

translating(i_text=final_hindi_list[232],true_o_text=final_english_list[232])

import nltk

bleu_s=[]
for count,list in enumerate(final_hindi_list4[300:700],300):
  o_text,t_o_text=translating(list,final_english_list4[count])
  hypothesis = making_reference_hypothesis(o_text)
  reference = making_reference_hypothesis(t_o_text)
  BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)
  bleu_s.append(BLEUscore)
  print(BLEUscore)

add=0
cnt=0
for no in bleu_s:
  cnt+=1
  add+=no
avg_bleu_score=add/cnt
print(avg_bleu_score)

translating(i_text=final_hindi_list[342],true_o_text=final_english_list4[342])

